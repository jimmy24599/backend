import type { ChannelFilters, ChannelSort, ChannelState, ExtendableGenerics, LiteralStringForUnion } from 'stream-chat';
import type { FileStateValue } from '../utils/utils';
export declare enum FileTypes {
    Audio = "audio",
    File = "file",
    Giphy = "giphy",
    Image = "image",
    Imgur = "imgur",
    Video = "video",
    VoiceRecording = "voiceRecording"
}
export type Asset = {
    duration: number;
    height: number;
    name: string;
    source: 'camera' | 'picker';
    type: string;
    uri: string;
    width: number;
    id?: string;
    mimeType?: string;
    originalUri?: string;
    size?: number;
};
export type File = {
    name: string;
    duration?: number;
    id?: string;
    mimeType?: string;
    originalUri?: string;
    size?: number;
    type?: FileTypes;
    uri?: string;
    waveform_data?: number[];
};
export type FileUpload = {
    file: File;
    id: string;
    state: FileStateValue;
    duration?: number;
    paused?: boolean;
    progress?: number;
    thumb_url?: string;
    type?: string;
    url?: string;
    waveform_data?: number[];
};
export type ImageUpload = {
    file: Partial<Asset>;
    id: string;
    state: FileStateValue;
    height?: number;
    url?: string;
    width?: number;
};
export type DefaultAttachmentType = UnknownType & {
    duration?: number;
    file_size?: number;
    mime_type?: string;
    originalFile?: File;
    originalImage?: Partial<Asset>;
    waveform_data?: number[];
};
export type Reaction = {
    id: string;
    name: string;
    type: string;
    image?: string;
};
interface DefaultUserType extends UnknownType {
    image?: string;
}
interface DefaultChannelType extends UnknownType {
    [key: string]: unknown;
    image?: string;
}
export interface DefaultStreamChatGenerics extends ExtendableGenerics {
    attachmentType: DefaultAttachmentType;
    channelType: DefaultChannelType;
    commandType: LiteralStringForUnion;
    eventType: UnknownType;
    memberType: UnknownType;
    messageType: UnknownType;
    reactionType: UnknownType;
    userType: DefaultUserType;
}
export type ChannelListEventListenerOptions<StreamChatGenerics extends DefaultStreamChatGenerics = DefaultStreamChatGenerics> = {
    filters?: ChannelFilters<StreamChatGenerics>;
    sort?: ChannelSort<StreamChatGenerics>;
};
export type UnknownType = Record<string, unknown>;
export type ValueOf<T> = T[keyof T];
export type ChannelUnreadState<StreamChatGenerics extends DefaultStreamChatGenerics = DefaultStreamChatGenerics> = Omit<ValueOf<ChannelState<StreamChatGenerics>['read']>, 'user'>;
export declare enum AndroidOutputFormat {
    DEFAULT = 0,
    THREE_GPP = 1,
    MPEG_4 = 2,
    AMR_NB = 3,
    AMR_WB = 4,
    AAC_ADIF = 5,
    AAC_ADTS = 6,
    RTP_AVP = 7,
    MPEG2TS = 8,
    WEBM = 9
}
export declare enum AndroidAudioEncoder {
    DEFAULT = 0,
    AMR_NB = 1,
    AMR_WB = 2,
    AAC = 3,
    HE_AAC = 4,
    AAC_ELD = 5
}
export declare enum IOSOutputFormat {
    LINEARPCM = "lpcm",
    AC3 = "ac-3",
    '60958AC3' = "cac3",
    APPLEIMA4 = "ima4",
    MPEG4AAC = "aac ",
    MPEG4CELP = "celp",
    MPEG4HVXC = "hvxc",
    MPEG4TWINVQ = "twvq",
    MACE3 = "MAC3",
    MACE6 = "MAC6",
    ULAW = "ulaw",
    ALAW = "alaw",
    QDESIGN = "QDMC",
    QDESIGN2 = "QDM2",
    QUALCOMM = "Qclp",
    MPEGLAYER1 = ".mp1",
    MPEGLAYER2 = ".mp2",
    MPEGLAYER3 = ".mp3",
    APPLELOSSLESS = "alac",
    MPEG4AAC_HE = "aach",
    MPEG4AAC_LD = "aacl",
    MPEG4AAC_ELD = "aace",
    MPEG4AAC_ELD_SBR = "aacf",
    MPEG4AAC_ELD_V2 = "aacg",
    MPEG4AAC_HE_V2 = "aacp",
    MPEG4AAC_SPATIAL = "aacs",
    AMR = "samr",
    AMR_WB = "sawb",
    AUDIBLE = "AUDB",
    ILBC = "ilbc",
    DVIINTELIMA = 1836253201,
    MICROSOFTGSM = 1836253233,
    AES3 = "aes3",
    ENHANCEDAC3 = "ec-3"
}
export declare enum IOSAudioQuality {
    MIN = 0,
    LOW = 32,
    MEDIUM = 64,
    HIGH = 96,
    MAX = 127
}
export type RecordingOptionsAndroid = {
    /**
     * The desired audio encoder. See the [`AndroidAudioEncoder`](#androidaudioencoder) enum for all valid values.
     */
    audioEncoder: AndroidAudioEncoder | number;
    /**
     * The desired file extension. Example valid values are `.3gp` and `.m4a`.
     * For more information, see the [Android docs](https://developer.android.com/guide/topics/media/media-formats)
     * for supported output formats.
     */
    extension: string;
    /**
     * The desired file format. See the [`AndroidOutputFormat`](#androidoutputformat) enum for all valid values.
     */
    outputFormat: AndroidOutputFormat | number;
    /**
     * The desired bit rate.
     *
     * Note that `prepareToRecordAsync()` may perform additional checks on the parameter to make sure whether the specified
     * bit rate is applicable, and sometimes the passed bitRate will be clipped internally to ensure the audio recording
     * can proceed smoothly based on the capabilities of the platform.
     *
     * @example `128000`
     */
    bitRate?: number;
    /**
     * The desired maximum file size in bytes, after which the recording will stop (but `stopAndUnloadAsync()` must still
     * be called after this point).
     *
     * @example `65536`
     */
    maxFileSize?: number;
    /**
     * The desired number of channels.
     *
     * Note that `prepareToRecordAsync()` may perform additional checks on the parameter to make sure whether the specified
     * number of audio channels are applicable.
     *
     * @example `1`, `2`
     */
    numberOfChannels?: number;
    /**
     * The desired sample rate.
     *
     * Note that the sampling rate depends on the format for the audio recording, as well as the capabilities of the platform.
     * For instance, the sampling rate supported by AAC audio coding standard ranges from 8 to 96 kHz,
     * the sampling rate supported by AMRNB is 8kHz, and the sampling rate supported by AMRWB is 16kHz.
     * Please consult with the related audio coding standard for the supported audio sampling rate.
     *
     * @example 44100
     */
    sampleRate?: number;
};
export type RecordingOptionsIOS = {
    /**
     * The desired audio quality. See the [`IOSAudioQuality`](#iosaudioquality) enum for all valid values.
     */
    audioQuality: IOSAudioQuality | number;
    /**
     * The desired bit rate.
     *
     * @example `128000`
     */
    bitRate: number;
    /**
     * The desired file extension.
     *
     * @example `'.caf'`
     */
    extension: string;
    /**
     * The desired number of channels.
     *
     * @example `1`, `2`
     */
    numberOfChannels: number;
    /**
     * The desired sample rate.
     *
     * @example `44100`
     */
    sampleRate: number;
    /**
     * The desired bit depth hint.
     *
     * @example `16`
     */
    bitDepthHint?: number;
    /**
     * The desired bit rate strategy. See the next section for an enumeration of all valid values of `bitRateStrategy`.
     */
    bitRateStrategy?: number;
    /**
     * The desired PCM bit depth.
     *
     * @example `16`
     */
    linearPCMBitDepth?: number;
    /**
     * A boolean describing if the PCM data should be formatted in big endian.
     */
    linearPCMIsBigEndian?: boolean;
    /**
     * A boolean describing if the PCM data should be encoded in floating point or integral values.
     */
    linearPCMIsFloat?: boolean;
    /**
     * The desired file format. See the [`IOSOutputFormat`](#iosoutputformat) enum for all valid values.
     */
    outputFormat?: string | IOSOutputFormat | number;
};
export type RecordingOptionsWeb = {
    bitsPerSecond?: number;
    mimeType?: string;
};
export type ExpoRecordingOptions = {
    /**
     * Recording options for the Android platform.
     */
    android: RecordingOptionsAndroid;
    /**
     * Recording options for the iOS platform.
     */
    ios: RecordingOptionsIOS;
    /**
     * Recording options for the Web platform.
     */
    web: RecordingOptionsWeb;
    /**
     * A boolean that determines whether audio level information will be part of the status object under the "metering" key.
     */
    isMeteringEnabled?: boolean;
    /**
     * A boolean that hints to keep the audio active after `prepareToRecordAsync` completes.
     * Setting this value can improve the speed at which the recording starts. Only set this value to `true` when you call `startAsync`
     * immediately after `prepareToRecordAsync`. This value is automatically set when using `Audio.recording.createAsync()`.
     */
    keepAudioActiveHint?: boolean;
};
export type AudioMode = {
    /**
     * A boolean selecting if recording is enabled on iOS.
     * > When this flag is set to `true`, playback may be routed to the phone earpiece instead of to the speaker. Set it back to `false` after stopping recording to reenable playback through the speaker.
     * @default false
     */
    allowsRecordingIOS: boolean;
    /**
     * An enum selecting how your experience's audio should interact with the audio from other apps on Android.
     */
    interruptionModeAndroid: InterruptionModeAndroid;
    /**
     * An enum selecting how your experience's audio should interact with the audio from other apps on iOS.
     */
    interruptionModeIOS: InterruptionModeIOS;
    /**
     * A boolean selecting if your experience's audio should play in silent mode on iOS.
     * @default false
     */
    playsInSilentModeIOS: boolean;
    /**
     * A boolean selecting if the audio is routed to earpiece on Android.
     * @default false
     */
    playThroughEarpieceAndroid: boolean;
    /**
     * A boolean selecting if your experience's audio should automatically be lowered in volume ("duck") if audio from another
     * app interrupts your experience. If `false`, audio from other apps will pause your audio.
     * @default true
     */
    shouldDuckAndroid: boolean;
    /**
     * A boolean selecting if the audio session (playback or recording) should stay active even when the app goes into background.
     * > This is not available in Expo Go for iOS, it will only work in standalone apps.
     * > To enable it for standalone apps, [follow the instructions below](#playing-or-recording-audio-in-background)
     * > to add `UIBackgroundModes` to your app configuration.
     * @default false
     */
    staysActiveInBackground: boolean;
};
export declare enum InterruptionModeIOS {
    /**
     * **This is the default option.** If this option is set, your experience's audio is mixed with audio playing in background apps.
     */
    MixWithOthers = 0,
    /**
     * If this option is set, your experience's audio interrupts audio from other apps.
     */
    DoNotMix = 1,
    /**
     * If this option is set, your experience's audio lowers the volume ("ducks") of audio from other apps while your audio plays.
     */
    DuckOthers = 2
}
export declare enum InterruptionModeAndroid {
    /**
     * If this option is set, your experience's audio interrupts audio from other apps.
     */
    DoNotMix = 1,
    /**
     * **This is the default option.** If this option is set, your experience's audio lowers the volume ("ducks") of audio from other apps while your audio plays.
     */
    DuckOthers = 2
}
export type ExpoAudioRecordingConfiguration = {
    mode?: Partial<AudioMode>;
    options?: Partial<ExpoRecordingOptions>;
};
export declare enum AudioSourceAndroidType {
    DEFAULT = 0,
    MIC = 1,
    VOICE_UPLINK = 2,
    VOICE_DOWNLINK = 3,
    VOICE_CALL = 4,
    CAMCORDER = 5,
    VOICE_RECOGNITION = 6,
    VOICE_COMMUNICATION = 7,
    REMOTE_SUBMIX = 8,
    UNPROCESSED = 9,
    RADIO_TUNER = 1998,
    HOTWORD = 1999
}
export declare enum OutputFormatAndroidType {
    DEFAULT = 0,
    THREE_GPP = 1,
    MPEG_4 = 2,
    AMR_NB = 3,
    AMR_WB = 4,
    AAC_ADIF = 5,
    AAC_ADTS = 6,
    OUTPUT_FORMAT_RTP_AVP = 7,
    MPEG_2_TS = 8,
    WEBM = 9
}
export declare enum AudioEncoderAndroidType {
    DEFAULT = 0,
    AMR_NB = 1,
    AMR_WB = 2,
    AAC = 3,
    HE_AAC = 4,
    AAC_ELD = 5,
    VORBIS = 6
}
export declare enum AVEncodingOption {
    aac = "aac",
    alac = "alac",
    alaw = "alaw",
    amr = "amr",
    flac = "flac",
    ima4 = "ima4",
    lpcm = "lpcm",
    MAC3 = "MAC3",
    MAC6 = "MAC6",
    mp1 = "mp1",
    mp2 = "mp2",
    mp4 = "mp4",
    opus = "opus",
    ulaw = "ulaw",
    wav = "wav"
}
export declare enum AVModeIOSOption {
    gamechat = "gamechat",
    measurement = "measurement",
    movieplayback = "movieplayback",
    spokenaudio = "spokenaudio",
    videochat = "videochat",
    videorecording = "videorecording",
    voicechat = "voicechat",
    voiceprompt = "voiceprompt"
}
export type AVModeIOSType = AVModeIOSOption.gamechat | AVModeIOSOption.measurement | AVModeIOSOption.movieplayback | AVModeIOSOption.spokenaudio | AVModeIOSOption.videochat | AVModeIOSOption.videorecording | AVModeIOSOption.voicechat | AVModeIOSOption.voiceprompt;
export declare enum AVEncoderAudioQualityIOSType {
    min = 0,
    low = 32,
    medium = 64,
    high = 96,
    max = 127
}
export declare enum AVLinearPCMBitDepthKeyIOSType {
    'bit8' = 8,
    'bit16' = 16,
    'bit24' = 24,
    'bit32' = 32
}
type AVEncodingType = AVEncodingOption.lpcm | AVEncodingOption.ima4 | AVEncodingOption.aac | AVEncodingOption.MAC3 | AVEncodingOption.MAC6 | AVEncodingOption.ulaw | AVEncodingOption.alaw | AVEncodingOption.mp1 | AVEncodingOption.mp2 | AVEncodingOption.mp4 | AVEncodingOption.alac | AVEncodingOption.amr | AVEncodingOption.flac | AVEncodingOption.opus | AVEncodingOption.wav;
export interface AudioSet {
    AudioChannelsAndroid?: number;
    AudioEncoderAndroid?: AudioEncoderAndroidType;
    AudioEncodingBitRateAndroid?: number;
    AudioSamplingRateAndroid?: number;
    AudioSourceAndroid?: AudioSourceAndroidType;
    AVEncoderAudioQualityKeyIOS?: AVEncoderAudioQualityIOSType;
    AVEncoderBitRateKeyIOS?: number;
    AVFormatIDKeyIOS?: AVEncodingType;
    AVLinearPCMBitDepthKeyIOS?: AVLinearPCMBitDepthKeyIOSType;
    AVLinearPCMIsBigEndianKeyIOS?: boolean;
    AVLinearPCMIsFloatKeyIOS?: boolean;
    AVLinearPCMIsNonInterleavedIOS?: boolean;
    AVModeIOS?: AVModeIOSType;
    AVNumberOfChannelsKeyIOS?: number;
    AVSampleRateKeyIOS?: number;
    OutputFormatAndroid?: OutputFormatAndroidType;
}
export type RNCLIRecordingOptions = {
    audioSet?: AudioSet;
    /**
     * A boolean that determines whether audio level information will be part of the status object under the "metering" key.
     */
    isMeteringEnabled?: boolean;
};
export type RNCLIAudioRecordingConfiguration = {
    options?: RNCLIRecordingOptions;
};
export {};
//# sourceMappingURL=types.d.ts.map